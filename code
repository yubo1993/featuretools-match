# -*- coding: utf-8 -*-
"""
Created on Wed Jan 23 10:37:47 2019

@author: admin
"""

import featuretools as ft
import pandas as pd
import gc
import lightgbm as lgb
import numpy as np
import warnings
import time
from sklearn.metrics import roc_auc_score
warnings.filterwarnings('ignore')
data_path='C:\\Users\\admin\\Downloads\\all\\'
#读取数据,为了方便看到模型效果，我们只选择部分数据
application_test_data=pd.read_csv(data_path+'application_test.csv').reset_index(drop = True).loc[:2, :]
application_train_data=pd.read_csv(data_path+'application_train.csv').sort_values('SK_ID_CURR').reset_index(drop = True).loc[:20000, :]
bureau_data=pd.read_csv(data_path+'bureau.csv').sort_values(['SK_ID_CURR', 'SK_ID_BUREAU']).reset_index(drop = True).loc[:20000, :]
bureau_balance_data=pd.read_csv(data_path+'bureau_balance.csv').sort_values('SK_ID_BUREAU').reset_index(drop = True).loc[:20000, :]
credit_card_balance_data=pd.read_csv(data_path+'credit_card_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:20000, :]
installments_payments_data=pd.read_csv(data_path+'installments_payments.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:20000, :]
POS_CASH_balance_data=pd.read_csv(data_path+'POS_CASH_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:20000, :]
previous_application_data=pd.read_csv(data_path+'previous_application.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:20000, :]

app_test=application_test_data
app_train=application_train_data
bureau=bureau_data
bureau_balance=bureau_balance_data
credit=credit_card_balance_data
installments=installments_payments_data
cash=POS_CASH_balance_data
previous=previous_application_data

del application_test_data,application_train_data,bureau_data,bureau_balance_data,credit_card_balance_data,installments_payments_data,POS_CASH_balance_data,previous_application_data


#combine train and test(maybe dataleak)
# Add identifying column
app_train['set'] = 'train'
app_test['set'] = 'test'
app_test["TARGET"] = np.nan
# Append the dataframes
app = app_train.append(app_test, ignore_index = True)
# Entity set with id applications
es = ft.EntitySet(id = 'clients')
# Entities with a unique index
es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR')
es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')
es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV')

# Entities that do not have a unique index
es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, 
                              make_index = True, index = 'bureaubalance_index')

es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, 
                              make_index = True, index = 'cash_index')

es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,
                              make_index = True, index = 'installments_index')

es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,
                              make_index = True, index = 'credit_index')

# Relationship between app and bureau
r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])

# Relationship between bureau and bureau balance
r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])

# Relationship between current app and previous apps
r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])

# Relationships between previous apps and cash, installments, and credit
r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])
r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])
r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])
# Add in the defined relationships
es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,
                           r_previous_cash, r_previous_installments, r_previous_credit])

es = es.add_relationships([r_app_bureau, r_bureau_balance])
# Print out the EntitySet
es
"""
# List the primitives in a dataframe
primitives = ft.list_primitives()
pd.options.display.max_colwidth = 100
primitives[primitives['type'] == 'aggregation'].head(10)
primitives[primitives['type'] == 'transform'].head(10)
"""
# Default primitives from featuretools
default_agg_primitives =  ['sum', 'count', 'min', 'max', 'mean', 'mode']
default_trans_primitives =  ['day', 'year', 'month']

start=time.time()
# DFS with list primitives
feature_matrix, feature_names = ft.dfs(entityset = es, target_entity = 'app',
                                       trans_primitives = default_trans_primitives,
                                       agg_primitives=default_agg_primitives, 
                                        max_depth = 1, features_only=False, verbose = True,n_jobs=1)
end=time.time()
#特征衍生运行时间
long=end-start

from featuretools import selection

# Remove features with only one unique value
feature_matrix2 = selection.remove_low_information_features(feature_matrix)

print('Removed %d features' % (feature_matrix.shape[1]- feature_matrix2.shape[1]))
# Separate out the train and test sets

target = 'TARGET'
predictors = ['AMT_ANNUITY',
 'AMT_CREDIT',
 'AMT_GOODS_PRICE',
 'AMT_INCOME_TOTAL',
 'AMT_REQ_CREDIT_BUREAU_DAY',
 'AMT_REQ_CREDIT_BUREAU_HOUR',
 'AMT_REQ_CREDIT_BUREAU_MON',
 'AMT_REQ_CREDIT_BUREAU_QRT',
 'AMT_REQ_CREDIT_BUREAU_WEEK',
 'AMT_REQ_CREDIT_BUREAU_YEAR',
 'APARTMENTS_AVG',
 'APARTMENTS_MEDI',
 'APARTMENTS_MODE',
 'BASEMENTAREA_AVG',
 'BASEMENTAREA_MEDI',
 'BASEMENTAREA_MODE',
 'CNT_CHILDREN',
 'CNT_FAM_MEMBERS',
 'CODE_GENDER',
 'COMMONAREA_AVG',
 'COMMONAREA_MEDI',
 'COMMONAREA_MODE',
 'DAYS_BIRTH',
 'DAYS_EMPLOYED',
 'DAYS_ID_PUBLISH',
 'DAYS_LAST_PHONE_CHANGE',
 'DAYS_REGISTRATION',
 'DEF_30_CNT_SOCIAL_CIRCLE',
 'DEF_60_CNT_SOCIAL_CIRCLE',
 'ELEVATORS_AVG',
 'ELEVATORS_MEDI',
 'ELEVATORS_MODE',
 'EMERGENCYSTATE_MODE',
 'ENTRANCES_AVG',
 'ENTRANCES_MEDI',
 'ENTRANCES_MODE',
 'EXT_SOURCE_1',
 'EXT_SOURCE_2',
 'EXT_SOURCE_3',
 'FLAG_CONT_MOBILE',
 'FLAG_DOCUMENT_11',
 'FLAG_DOCUMENT_13',
 'FLAG_DOCUMENT_14',
 'FLAG_DOCUMENT_15',
 'FLAG_DOCUMENT_16',
 'FLAG_DOCUMENT_18',
 'FLAG_DOCUMENT_3',
 'FLAG_DOCUMENT_5',
 'FLAG_DOCUMENT_6',
 'FLAG_DOCUMENT_8',
 'FLAG_DOCUMENT_9',
 'FLAG_EMAIL',
 'FLAG_EMP_PHONE',
 'FLAG_OWN_CAR',
 'FLAG_OWN_REALTY',
 'FLAG_PHONE',
 'FLAG_WORK_PHONE',
 'FLOORSMAX_AVG',
 'FLOORSMAX_MEDI',
 'FLOORSMAX_MODE',
 'FLOORSMIN_AVG',
 'FLOORSMIN_MEDI',
 'FLOORSMIN_MODE',
 'FONDKAPREMONT_MODE',
 'HOUR_APPR_PROCESS_START',
 'HOUSETYPE_MODE',
 'LANDAREA_AVG',
 'LANDAREA_MEDI',
 'LANDAREA_MODE',
 'LIVE_CITY_NOT_WORK_CITY',
 'LIVE_REGION_NOT_WORK_REGION',
 'LIVINGAPARTMENTS_AVG',
 'LIVINGAPARTMENTS_MEDI',
 'LIVINGAPARTMENTS_MODE',
 'LIVINGAREA_AVG',
 'LIVINGAREA_MEDI',
 'LIVINGAREA_MODE',
 'NAME_CONTRACT_TYPE',
 'NAME_EDUCATION_TYPE',
 'NAME_FAMILY_STATUS',
 'NAME_HOUSING_TYPE',
 'NAME_INCOME_TYPE',
 'NAME_TYPE_SUITE',
 'NONLIVINGAPARTMENTS_AVG',
 'NONLIVINGAPARTMENTS_MEDI',
 'NONLIVINGAPARTMENTS_MODE',
 'NONLIVINGAREA_AVG',
 'NONLIVINGAREA_MEDI',
 'NONLIVINGAREA_MODE',
 'OBS_30_CNT_SOCIAL_CIRCLE',
 'OBS_60_CNT_SOCIAL_CIRCLE',
 'OCCUPATION_TYPE',
 'ORGANIZATION_TYPE',
 'OWN_CAR_AGE',
 'REGION_POPULATION_RELATIVE',
 'REGION_RATING_CLIENT',
 'REGION_RATING_CLIENT_W_CITY',
 'REG_CITY_NOT_LIVE_CITY',
 'REG_CITY_NOT_WORK_CITY',
 'REG_REGION_NOT_LIVE_REGION',
 'REG_REGION_NOT_WORK_REGION',
 'TOTALAREA_MODE',
 'WALLSMATERIAL_MODE',
 'WEEKDAY_APPR_PROCESS_START',
 'YEARS_BEGINEXPLUATATION_AVG',
 'YEARS_BEGINEXPLUATATION_MEDI',
 'YEARS_BEGINEXPLUATATION_MODE',
 'YEARS_BUILD_AVG',
 'YEARS_BUILD_MEDI',
 'YEARS_BUILD_MODE',
 'SUM(bureau.DAYS_CREDIT)',
 'SUM(bureau.CREDIT_DAY_OVERDUE)',
 'SUM(bureau.DAYS_CREDIT_ENDDATE)',
 'SUM(bureau.DAYS_ENDDATE_FACT)',
 'SUM(bureau.AMT_CREDIT_MAX_OVERDUE)',
 'SUM(bureau.CNT_CREDIT_PROLONG)',
 'SUM(bureau.AMT_CREDIT_SUM)',
 'SUM(bureau.AMT_CREDIT_SUM_DEBT)',
 'SUM(bureau.AMT_CREDIT_SUM_LIMIT)',
 'SUM(bureau.AMT_CREDIT_SUM_OVERDUE)',
 'SUM(bureau.DAYS_CREDIT_UPDATE)',
 'SUM(bureau.AMT_ANNUITY)',
 'COUNT(bureau)',
 'MIN(bureau.DAYS_CREDIT)',
 'MIN(bureau.CREDIT_DAY_OVERDUE)',
 'MIN(bureau.DAYS_CREDIT_ENDDATE)',
 'MIN(bureau.DAYS_ENDDATE_FACT)',
 'MIN(bureau.AMT_CREDIT_MAX_OVERDUE)',
 'MIN(bureau.CNT_CREDIT_PROLONG)',
 'MIN(bureau.AMT_CREDIT_SUM)',
 'MIN(bureau.AMT_CREDIT_SUM_DEBT)',
 'MIN(bureau.AMT_CREDIT_SUM_LIMIT)',
 'MIN(bureau.AMT_CREDIT_SUM_OVERDUE)',
 'MIN(bureau.DAYS_CREDIT_UPDATE)',
 'MIN(bureau.AMT_ANNUITY)',
 'MAX(bureau.DAYS_CREDIT)',
 'MAX(bureau.CREDIT_DAY_OVERDUE)',
 'MAX(bureau.DAYS_CREDIT_ENDDATE)',
 'MAX(bureau.DAYS_ENDDATE_FACT)',
 'MAX(bureau.AMT_CREDIT_MAX_OVERDUE)',
 'MAX(bureau.CNT_CREDIT_PROLONG)',
 'MAX(bureau.AMT_CREDIT_SUM)',
 'MAX(bureau.AMT_CREDIT_SUM_DEBT)',
 'MAX(bureau.AMT_CREDIT_SUM_LIMIT)',
 'MAX(bureau.AMT_CREDIT_SUM_OVERDUE)',
 'MAX(bureau.DAYS_CREDIT_UPDATE)',
 'MAX(bureau.AMT_ANNUITY)',
 'MEAN(bureau.DAYS_CREDIT)',
 'MEAN(bureau.CREDIT_DAY_OVERDUE)',
 'MEAN(bureau.DAYS_CREDIT_ENDDATE)',
 'MEAN(bureau.DAYS_ENDDATE_FACT)',
 'MEAN(bureau.AMT_CREDIT_MAX_OVERDUE)',
 'MEAN(bureau.CNT_CREDIT_PROLONG)',
 'MEAN(bureau.AMT_CREDIT_SUM)',
 'MEAN(bureau.AMT_CREDIT_SUM_DEBT)',
 'MEAN(bureau.AMT_CREDIT_SUM_LIMIT)',
 'MEAN(bureau.AMT_CREDIT_SUM_OVERDUE)',
 'MEAN(bureau.DAYS_CREDIT_UPDATE)',
 'MEAN(bureau.AMT_ANNUITY)',
 'MODE(bureau.CREDIT_ACTIVE)',
 'MODE(bureau.CREDIT_CURRENCY)',
 'MODE(bureau.CREDIT_TYPE)']
categorical = ['CODE_GENDER', 'EMERGENCYSTATE_MODE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',
       'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'NAME_CONTRACT_TYPE',
       'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',
       'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE',
       'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START',
       'MODE(bureau.CREDIT_ACTIVE)', 'MODE(bureau.CREDIT_CURRENCY)',
       'MODE(bureau.CREDIT_TYPE)']

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
for col in categorical:
    feature_matrix2[col] = label_encoder.fit_transform(np.array(feature_matrix2[col].astype(str)).reshape((-1,)))


train = feature_matrix2[feature_matrix2['set'] == 'train']
test = feature_matrix2[feature_matrix2['set'] == 'test']

# Align dataframes on the columns
train, test = train.align(test, join = 'inner', axis = 1)
test = test.drop(columns = ['TARGET','set'])
train = train.drop(columns = ['set'])


print('Final Training Shape: ', train.shape)
print('Final Testing Shape: ', test.shape)
#下采样
import imblearn
from imblearn.under_sampling import RandomUnderSampler 
rus = RandomUnderSampler(random_state=42)
cols=train.drop(['TARGET'],axis=1).columns.tolist()
X_res, y_res = rus.fit_resample(train.drop(['TARGET'],axis=1),train['TARGET'])
X_res=pd.DataFrame(X_res,columns=cols)
train=pd.concat([X_res,pd.DataFrame(y_res,columns=['TARGET'])],axis=1)
from sklearn.model_selection import train_test_split
dtrain,dvalid=train_test_split(train,train_size=0.7,random_state=41)
def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',
                 feval=None, early_stopping_rounds=100, num_boost_round=3000, verbose_eval=10, categorical_features=None):
    lgb_params = {
        'boosting_type': 'gbdt',
        'objective': objective,
        'metric':metrics,
        'learning_rate': 0.05,
        'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)
        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)
        'max_depth': -1,  # -1 means no limit
        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)
        'max_bin': 255,  # Number of bucketed bin for feature values
        'subsample': 0.7,  # Subsample ratio of the training instance.
        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable
        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.
        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)
        'subsample_for_bin': 200000,  # Number of samples for constructing bin
        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization
        'reg_alpha': 1,  # L1 regularization term on weights
        'reg_lambda': 5,  # L2 regularization term on weights
        'nthread': 8,
        'verbose': 1,
        
        'tree_learner':'data'
    }

    lgb_params.update(params)

    print("preparing validation datasets")

    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,
                          feature_name=predictors,
                          categorical_feature=categorical_features
                          )
    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,
                          feature_name=predictors,
                          categorical_feature=categorical_features
                          )

    evals_results = {}

    bst1 = lgb.train(lgb_params, 
                     xgtrain, 
                     valid_sets=[xgtrain, xgvalid], 
                     valid_names=['train','valid'], 
                     evals_result=evals_results, 
                     num_boost_round=num_boost_round,
                     early_stopping_rounds=early_stopping_rounds,
                     verbose_eval=10, 
                     feval=feval)

    n_estimators = bst1.best_iteration
    print("\nModel Report")
    print("n_estimators : ", n_estimators)
    print(metrics+":", evals_results['valid'][metrics][n_estimators-1])

    return bst1





params = {
    'learning_rate': 0.1,
    #'is_unbalance': 'true', # replaced with scale_pos_weight argument
    'num_leaves': 5,  # we should let it be smaller than 2^(max_depth)
    'max_depth': -1,  # -1 means no limit
    'min_child_samples': 30,  # Minimum number of data need in a child(min_data_in_leaf)
    'max_bin': 255,  # Number of bucketed bin for feature values
    'subsample': 0.8,  # Subsample ratio of the training instance.
    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable
    'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.
    'min_child_weight':20,  # Minimum sum of instance weight(hessian) needed in a child(leaf)
    #'scale_pos_weight':400 # because training data is extremely unbalanced 
}
bst = lgb_modelfit_nocv(params, 
                        dtrain, 
                        dvalid, 
                        predictors, 
                        target, 
                        objective='binary', 
                        metrics='auc',
                        early_stopping_rounds=200, 
                        verbose_eval=True, 
                        num_boost_round=400, 
                        categorical_features=categorical)


















